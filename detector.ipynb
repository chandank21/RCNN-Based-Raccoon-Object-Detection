{"cells":[{"cell_type":"code","execution_count":null,"id":"356d50cf","metadata":{"id":"356d50cf"},"outputs":[],"source":["import os\n","import pandas as pd\n","import cv2"]},{"cell_type":"markdown","id":"a9c17e6b","metadata":{"id":"a9c17e6b"},"source":["### Dataset preparation"]},{"cell_type":"code","execution_count":null,"id":"bc82b90c","metadata":{"id":"bc82b90c"},"outputs":[],"source":["path = r\"C:\\Users\\chakumar\\CV Projects\\datasets\\Racoon Images\\images\""]},{"cell_type":"code","execution_count":null,"id":"e7e7c571","metadata":{"id":"e7e7c571"},"outputs":[],"source":["imPaths = os.listdir(path)"]},{"cell_type":"code","execution_count":null,"id":"f7fff589","metadata":{"id":"f7fff589"},"outputs":[],"source":["label_df = pd.read_csv(r\"C:\\Users\\chakumar\\CV Projects\\datasets\\train_labels_.csv\")\n","label_df.set_index('filename',inplace = True)"]},{"cell_type":"code","execution_count":null,"id":"b27c15d3","metadata":{"id":"b27c15d3"},"outputs":[],"source":["positive_path = r\"C:\\Users\\chakumar\\CV Projects\\Object detection RCNN\\racoons\"\n","negative_path = r\"C:\\Users\\chakumar\\CV Projects\\Object detection RCNN\\notRacoons\""]},{"cell_type":"code","execution_count":null,"id":"1534ac7c","metadata":{"id":"1534ac7c"},"outputs":[],"source":["def compute_iou(boxA, boxB):\n","    # determine the (x, y)-coordinates of the intersection rectangle\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","    # compute the area of intersection rectangle\n","    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","    # compute the area of both the prediction and ground-truth\n","    # rectangles\n","    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n","    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n","    # compute the intersection over union by taking the intersection\n","    # area and dividing it by the sum of prediction + ground-truth\n","    # areas - the intersection area\n","    iou = interArea / float(boxAArea + boxBArea - interArea)\n","    # return the intersection over union value\n","    return iou"]},{"cell_type":"code","execution_count":null,"id":"28713a76","metadata":{"id":"28713a76"},"outputs":[],"source":["max_proposed_rois = 2000\n","MAX_POSITIVE = 30\n","MAX_NEGATIVE = 10\n","input_size = (224, 224)\n","\n","total_positive_roi = 0\n","total_negative_roi = 0\n","rois_dict  = dict(columns=[\"filename\",\"rois\"])\n","pos_paths = []\n","pos_rois = []\n","\n","for file in imPaths:\n","    img_file = f\"{path}\\{file}\"\n","    img = cv2.imread(img_file)\n","\n","    if file not in label_df.index:\n","        continue\n","\n","    gtBoxes = []\n","    ground_truth_rois = label_df.loc[f\"{file}\"][[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n","    if ground_truth_rois.ndim == 1:\n","        gtBoxes.append(ground_truth_rois)\n","    else:\n","        gtBoxes = ground_truth_rois\n","\n","    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","    ss.setBaseImage(img)\n","    ss.switchToSelectiveSearchFast()\n","    rects = ss.process()\n","\n","    proposed_roi = []\n","    for (x, y, w, h) in rects:\n","        proposed_roi.append((x, y, x + w, y + h))\n","\n","    positive_roi, negative_roi = 0,0\n","\n","    for roi in proposed_roi[:max_proposed_rois]:\n","        (propStartX, propStartY, propEndX, propEndY) = roi\n","\n","        for gtbox in gtBoxes:\n","            iou = compute_iou(roi, gtbox)\n","            proposed_img = None\n","\n","            if iou > 0.7 and positive_roi < MAX_POSITIVE:\n","                proposed_img = img[propStartY:propEndY, propStartX:propEndX]\n","                outputpath = f\"{positive_path}\\{total_positive_roi}.jpg\"\n","\n","                positive_roi = positive_roi + 1\n","                total_positive_roi = total_positive_roi + 1\n","\n","                pos_paths.append(outputpath)\n","                pos_rois.append(roi)\n","\n","            overlap = propStartX >= gtbox[0]\n","            overlap = overlap and propStartY >= gtbox[1]\n","            overlap = overlap and propEndX <= gtbox[2]\n","            overlap = overlap and propEndY <= gtbox[3]\n","\n","            if not overlap and iou < 0.05 and negative_roi < MAX_NEGATIVE:\n","                proposed_img = img[propStartY:propEndY, propStartX:propEndX]\n","                negative_roi = negative_roi + 1\n","                total_negative_roi = total_negative_roi + 1\n","                outputpath = f\"{negative_path}\\{total_negative_roi}.jpg\"\n","\n","            if proposed_img is not None:\n","                cv2.resize(proposed_img, input_size, interpolation=cv2.INTER_CUBIC)\n","                cv2.imwrite(outputpath, proposed_img)\n","\n","rois_dict['filename'] = pos_paths\n","rois_dict['rois'] = pos_rois"]},{"cell_type":"markdown","id":"d76a7720","metadata":{"id":"d76a7720"},"source":["### Fine Tuning"]},{"cell_type":"code","execution_count":null,"id":"eac56b1c","metadata":{"id":"eac56b1c"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import AveragePooling2D\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","import pickle\n"]},{"cell_type":"code","execution_count":null,"id":"8e0b8e5f","metadata":{"id":"8e0b8e5f"},"outputs":[],"source":["from imutils import paths"]},{"cell_type":"code","execution_count":null,"id":"6173ca26","metadata":{"id":"6173ca26"},"outputs":[],"source":["impaths = list(paths.list_images(r\"C:\\Users\\chakumar\\CV Projects\\Object detection RCNN\\new_dataset\"))\n","images, labels = [], []\n","\n","for impath in impaths:\n","    img = cv2.imread(impath)\n","    img = cv2.resize(img, (224, 224))\n","    img = preprocess_input(img)\n","    images.append(img)\n","    label = impath.split(os.path.sep)[-2]\n","    labels.append(label)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"118e6a20","metadata":{"id":"118e6a20"},"outputs":[],"source":["images = np.array(images, dtype = \"float64\")\n","labels = np.array(labels)"]},{"cell_type":"code","execution_count":null,"id":"5a54528b","metadata":{"id":"5a54528b"},"outputs":[],"source":["lb = LabelBinarizer()\n","labels = lb.fit_transform(labels)\n","labels = to_categorical(labels)"]},{"cell_type":"code","execution_count":null,"id":"11bb7659","metadata":{"id":"11bb7659"},"outputs":[],"source":["(trainX, testX, trainY, testY) = train_test_split(images, labels,\n","    test_size=0.20, stratify=labels, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"c9a51854","metadata":{"id":"c9a51854"},"outputs":[],"source":["aug = ImageDataGenerator(rotation_range=20,\n","                         zoom_range=0.15,\n","                         width_shift_range=0.2,\n","                        height_shift_range=0.2,\n","                         shear_range=0.15,\n","                         horizontal_flip=True,\n","                         fill_mode=\"nearest\")\n"]},{"cell_type":"code","execution_count":null,"id":"9da9dd09","metadata":{"id":"9da9dd09"},"outputs":[],"source":["def classifer():\n","    baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n","    input_tensor=Input(shape=(224, 224, 3)))\n","    # the base model\n","    headModel = baseModel.output\n","    headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n","    headModel = Flatten(name=\"flatten\")(headModel)\n","    headModel = Dense(128, activation=\"relu\")(headModel)\n","    headModel = Dropout(0.5)(headModel)\n","    headModel = Dense(2, activation=\"softmax\")(headModel)\n","\n","    model = Model(inputs=baseModel.input, outputs=headModel)\n","    # loop over all layers in the base model and freeze them so they will\n","    for layer in baseModel.layers:\n","        layer.trainable = False\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"aec91dd3","metadata":{"id":"aec91dd3","outputId":"d89ad9d1-4572-410c-96af-9fdc809f4d0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n","Epoch 1/10\n","36/36 [==============================] - 42s 1s/step - loss: 0.4358 - accuracy: 0.8335 - val_loss: 0.1399 - val_accuracy: 0.9858\n","Epoch 2/10\n","36/36 [==============================] - 42s 1s/step - loss: 0.1202 - accuracy: 0.9836 - val_loss: 0.0569 - val_accuracy: 0.9876\n","Epoch 3/10\n","36/36 [==============================] - 40s 1s/step - loss: 0.0649 - accuracy: 0.9907 - val_loss: 0.0348 - val_accuracy: 0.9912\n","Epoch 4/10\n","36/36 [==============================] - 39s 1s/step - loss: 0.0428 - accuracy: 0.9947 - val_loss: 0.0257 - val_accuracy: 0.9929\n","Epoch 5/10\n","36/36 [==============================] - 39s 1s/step - loss: 0.0343 - accuracy: 0.9938 - val_loss: 0.0209 - val_accuracy: 0.9929\n","Epoch 6/10\n","36/36 [==============================] - 40s 1s/step - loss: 0.0283 - accuracy: 0.9938 - val_loss: 0.0187 - val_accuracy: 0.9965\n","Epoch 7/10\n","36/36 [==============================] - 39s 1s/step - loss: 0.0238 - accuracy: 0.9942 - val_loss: 0.0172 - val_accuracy: 0.9965\n","Epoch 8/10\n","36/36 [==============================] - 41s 1s/step - loss: 0.0207 - accuracy: 0.9973 - val_loss: 0.0138 - val_accuracy: 0.9947\n","Epoch 9/10\n","36/36 [==============================] - 42s 1s/step - loss: 0.0191 - accuracy: 0.9947 - val_loss: 0.0140 - val_accuracy: 0.9965\n","Epoch 10/10\n","36/36 [==============================] - 42s 1s/step - loss: 0.0158 - accuracy: 0.9956 - val_loss: 0.0123 - val_accuracy: 0.9965\n"]}],"source":["classifier_model = classifer()\n","classifier_model.compile(optimizer = Adam(learning_rate = 0.0001), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n","model = classifier_model.fit(aug.flow(trainX, trainY, batch_size = 64),validation_data=(testX, testY), epochs = 10)"]},{"cell_type":"code","execution_count":null,"id":"67c37104","metadata":{"id":"67c37104","outputId":"cad42092-6fd0-4ec1-a718-0662ea1cea83"},"outputs":[{"name":"stdout","output_type":"stream","text":["18/18 [==============================] - 5s 253ms/step\n"]}],"source":["predicts = classifier_model.predict(testX)\n","predicts = np.argmax(predicts, axis = 1)"]},{"cell_type":"code","execution_count":null,"id":"47fa8c55","metadata":{"id":"47fa8c55","outputId":"b291e986-8fc4-4bfa-da13-1ac9dc893382"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","  notRacoons       1.00      0.99      1.00       320\n","     racoons       0.99      1.00      1.00       245\n","\n","    accuracy                           1.00       565\n","   macro avg       1.00      1.00      1.00       565\n","weighted avg       1.00      1.00      1.00       565\n","\n"]}],"source":["print(classification_report(testY.argmax(axis=1), predicts,\n"," target_names=lb.classes_))"]},{"cell_type":"code","execution_count":null,"id":"d199cd99","metadata":{"id":"d199cd99"},"outputs":[],"source":["classifier_model.save(r\"C:\\Users\\chakumar\\CV Projects\\Object detection RCNN\\classifiermodel\", save_format=\"h5\")\n","# serialize the label encoder to disk\n","\n","f = open(r\"C:\\Users\\chakumar\\CV Projects\\Object detection RCNN\\bl\", \"wb\")\n","f.write(pickle.dumps(lb))\n","f.close()"]},{"cell_type":"markdown","id":"66c26f81","metadata":{"id":"66c26f81"},"source":["### Inferencing"]},{"cell_type":"code","execution_count":null,"id":"c1d6b79c","metadata":{"id":"c1d6b79c"},"outputs":[],"source":["from imutils.object_detection import non_max_suppression\n","from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":null,"id":"512a1e10","metadata":{"id":"512a1e10"},"outputs":[],"source":["model  =  load_model(r\"C:\\Users\\chakumar\\CV Projects\\Object detection RCNN\\classifiermodel\")\n","lb = pickle.loads(open(r\"C:\\Users\\chakumar\\CV Projects\\Object detection RCNN\\bl\", \"rb\").read())"]},{"cell_type":"code","execution_count":null,"id":"0c67c263","metadata":{"id":"0c67c263","outputId":"e0378897-4fd6-446a-998d-4b95a73c4161"},"outputs":[{"name":"stdout","output_type":"stream","text":["45/45 [==============================] - 12s 272ms/step\n"]}],"source":["raccon1 = cv2.imread(r\"C:\\Users\\chakumar\\CV Projects\\datasets\\Racoon Images\\images\\raccoon-1.jpg\")\n","\n","ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","ss.setBaseImage(raccon1)\n","ss.switchToSelectiveSearchFast()\n","rects = ss.process()\n","\n","proposals = []\n","boxes = []\n","for (x,y,w,h) in rects:\n","    roi = raccon1[y:y+h, x:x+w]\n","    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n","    roi = cv2.resize(roi, (224,224))\n","\n","    roi = preprocess_input(roi)\n","    proposals.append(roi)\n","    boxes.append((x,y,x+w,y+h))\n","\n","proposals = np.array(proposals, dtype = 'float64')\n","boxes = np.array(boxes, dtype = \"int32\")\n","\n","predictions = model.predict(proposals)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"794da75e","metadata":{"id":"794da75e"},"outputs":[],"source":["roi_labels = lb.classes_[np.argmax(predictions, axis = 1)]\n","racoons_idx = np.where(roi_labels == \"racoons\")[0]"]},{"cell_type":"code","execution_count":null,"id":"c1ddc88c","metadata":{"id":"c1ddc88c"},"outputs":[],"source":["racoons_boxes = boxes[racoons_idx]\n","racoons_probs = predictions[racoons_idx][:,-1]\n","\n","min_porb = 0.9999\n","racoons_idx_valid = np.where(racoons_probs > min_porb)[0]\n","racoons_boxes = racoons_boxes[racoons_idx_valid]\n","racoons_probs = racoons_probs[racoons_idx_valid]"]},{"cell_type":"code","execution_count":null,"id":"e7fde10c","metadata":{"id":"e7fde10c","outputId":"cb4498a9-19f2-44fd-a4e7-963cd222e66e"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":216,"metadata":{},"output_type":"execute_result"}],"source":["nms_img = raccon1.copy()\n","nmsbox = non_max_suppression(racoons_boxes, racoons_probs)\n","for idx in range(nmsbox.shape[0]):\n","    Xin,Yin,Xend,Yend = nmsbox[idx]\n","\n","    cv2.rectangle(nms_img, (Xin, Yin), (Xend, Yend),(0, 255, 0), 2)\n","\n","    text= f\"Raccoon:{idx}\"\n","\n","    cv2.putText(nms_img, text, (Xin, Yin),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n","\n","cv2.imwrite(\"nms_img.jpg\", nms_img)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}